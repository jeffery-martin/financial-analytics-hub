{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62837295",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit, explode, array, struct, sum as spark_sum, concat\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, StringType\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"RLM Revenue Refresh Started at {start_time}\")\n",
    "\n",
    "try:\n",
    "    # -----------------------------------\n",
    "    # STEP 1: Wake up the cluster\n",
    "    spark.range(1).collect()\n",
    "    print(\"‚úÖ Cluster is awake. Proceeding with SQL queries...\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # STEP 2: Extract weekly revenue data for Region1 and Region2 from SAP using Spark SQL\n",
    "\n",
    "    # --- Region1 ---\n",
    "    region1_df = spark.sql(\"\"\" \n",
    "      WITH ranked_data AS (\n",
    "      SELECT \n",
    "        geography_name,\n",
    "        fiscalyear,\n",
    "        fiscalquarter,\n",
    "        fiscal_week_in_qtr_num,\n",
    "        product_group,\n",
    "        revenue,\n",
    "        LoadDate,\n",
    "        CAST(LoadDate AS DATE) AS LoadDay,\n",
    "        ROW_NUMBER() OVER (\n",
    "          PARTITION BY geography_name, fiscalyear, fiscalquarter, fiscal_week_in_qtr_num, product_group, CAST(LoadDate AS DATE)\n",
    "          ORDER BY LoadDate DESC\n",
    "        ) AS rn\n",
    "      FROM analytics.gold.global_revenue\n",
    "      WHERE geography_name = 'Total Region1'\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "      'Region1' as Category,\n",
    "      d.FISCALYEAR as FiscalYear,\n",
    "      d.FISCALQUARTER as FiscalQuarter,\n",
    "      DATEADD(r.LoadDay, -1) AS Date,\n",
    "      SUM(r.revenue) AS Revenue\n",
    "    FROM ranked_data AS r\n",
    "    INNER JOIN analytics.gold.date AS d\n",
    "      ON DATEADD(r.LoadDay, -1) = d.day\n",
    "      AND r.fiscalyear = d.FISCALYEAR\n",
    "      AND r.fiscalquarter = d.FISCALQUARTER\n",
    "    WHERE r.rn = 1\n",
    "    GROUP BY \n",
    "      r.geography_name,\n",
    "      d.FISCALYEAR,\n",
    "      d.FISCALQUARTER,\n",
    "      d.FISCALQUARTER_WORK_DAY_NO,\n",
    "      r.LoadDay\n",
    "    ORDER BY Date DESC;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    # --- Region2 ---\n",
    "    Region2_df = spark.sql(\"\"\"\n",
    "      WITH latest_loads AS (\n",
    "      SELECT \n",
    "        CAST(LoadDate AS DATE) AS LoadDay,\n",
    "        MAX(LoadDate) AS LatestLoadDate\n",
    "      FROM analytics.gold.global_revenue\n",
    "      WHERE geography_name = 'Region2'\n",
    "      GROUP BY CAST(LoadDate AS DATE)\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "      r.geography_name as Category,\n",
    "      r.fiscalyear as FiscalYear,\n",
    "      r.fiscalquarter as FiscalQuarter,\n",
    "      DATEADD(CAST(r.LoadDate AS DATE), -1) AS Date,\n",
    "      SUM(r.revenue) AS Revenue\n",
    "    FROM analytics.gold.global_revenue r\n",
    "    JOIN latest_loads l\n",
    "      ON CAST(r.LoadDate AS DATE) = l.LoadDay\n",
    "      AND r.LoadDate = l.LatestLoadDate\n",
    "    INNER JOIN analytics.gold.date AS d\n",
    "      ON DATEADD(r.LoadDate, -1) = d.day\n",
    "      AND r.fiscalyear = d.FISCALYEAR\n",
    "      AND r.fiscalquarter = d.FISCALQUARTER\n",
    "    WHERE r.geography_name = 'Region2'\n",
    "    GROUP BY \n",
    "      r.geography_name,\n",
    "      r.fiscalyear,\n",
    "      r.fiscalquarter,\n",
    "      CAST(r.LoadDate AS DATE)\n",
    "    ORDER BY Date DESC;\n",
    "    \"\"\")\n",
    "\n",
    "    # Ensure data was returned from SAP for both regions\n",
    "    assert Region1_df is not None and Region1_df.count() > 0, \"‚ùå Region1_df is missing or empty!\"\n",
    "    assert Region2_df is not None and Region2_df.count() > 0, \"‚ùå Region2_df is missing or empty!\"\n",
    "    print(\"‚úÖ SQL queries completed successfully.\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # STEP 3: Load historical revenue from Excel for edge cases (FY25 Q1 & Q2)\n",
    "    # This step loads pre-processed values maintained by the Finance team until our SAP data catches up.\n",
    "    try:\n",
    "        import openpyxl\n",
    "    except ImportError:\n",
    "        %pip install openpyxl\n",
    "        import openpyxl\n",
    "        dbutils.library.restartPython()\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    excel_path = \"/Workspace/Users/financial-analytics-hub/R12_PY.xlsx\"\n",
    "\n",
    "    Region1_pd = pd.read_excel(excel_path, sheet_name=\"Region1\")\n",
    "    Region2_pd = pd.read_excel(excel_path, sheet_name=\"Region2\")\n",
    "\n",
    "    Region1_excel_raw = spark.createDataFrame(Region1_pd)\n",
    "    Region2_excel_raw = spark.createDataFrame(Region2_pd)\n",
    "\n",
    "    print(\"‚úÖ Excel files read successfully.\")\n",
    "\n",
    "    # Pivoting Excel data: Convert from wide format (dates as columns) to tall format (dates as rows) \n",
    "    date_columns = [col_name for col_name in Region1_excel_raw.columns if col_name != 'Category']\n",
    "\n",
    "    Region1_excel_melted = Region1_excel_raw.select(\n",
    "        explode(array(*[struct(lit(c).alias(\"Date\"), col(c).alias(\"Revenue\")) for c in date_columns])).alias(\"x\")\n",
    "    ).select(\n",
    "        col(\"x.Date\").cast(\"date\").alias(\"Date\"),\n",
    "        col(\"x.Revenue\").cast(\"double\").alias(\"Revenue\"),\n",
    "        lit(\"Region1\").alias(\"Category\")\n",
    "    )\n",
    "\n",
    "    Region2_excel_melted = Region2_excel_raw.select(\n",
    "        explode(array(*[struct(lit(c).alias(\"Date\"), col(c).alias(\"Revenue\")) for c in date_columns])).alias(\"x\")\n",
    "    ).select(\n",
    "        col(\"x.Date\").cast(\"date\").alias(\"Date\"),\n",
    "        col(\"x.Revenue\").cast(\"double\").alias(\"Revenue\"),\n",
    "        lit(\"Region2\").alias(\"Category\")\n",
    "    )\n",
    "\n",
    "    # Add fiscal calendar context to Excel data to match SAP output format\n",
    "    date_table_df = spark.table(\"analytics.gold.date\")\n",
    "\n",
    "    Region1_excel_joined = (Region1_excel_melted\n",
    "        .join(date_table_df.select(\"Date\", \"FiscalYear\", \"FiscalQuarter\"), \"Date\", \"left\")\n",
    "        .withColumn(\"FiscalYear\", concat(lit(\"FY\"), col(\"FiscalYear\").cast(\"string\"))))\n",
    "\n",
    "    Region2_excel_joined = (Region2_excel_melted\n",
    "        .join(date_table_df.select(\"Date\", \"FiscalYear\", \"FiscalQuarter\"), \"Date\", \"left\")\n",
    "        .withColumn(\"FiscalYear\", concat(lit(\"FY\"), col(\"FiscalYear\").cast(\"string\"))))\n",
    "\n",
    "    # Calculate QTD revenue for Excel-based data\n",
    "    window_spec = Window.partitionBy(\"FiscalYear\", \"FiscalQuarter\", \"Category\").orderBy(\"Date\") \\\n",
    "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    Region1_excel_qtd = (Region1_excel_joined\n",
    "        .withColumn(\"Revenue_QTD\", spark_sum(\"Revenue\").over(window_spec))\n",
    "        .select(\"Date\", \"FiscalYear\", \"FiscalQuarter\", col(\"Revenue_QTD\").alias(\"Revenue\"), \"Category\"))\n",
    "\n",
    "    Region2_excel_qtd = (Region2_excel_joined\n",
    "        .withColumn(\"Revenue_QTD\", spark_sum(\"Revenue\").over(window_spec))\n",
    "        .select(\"Date\", \"FiscalYear\", \"FiscalQuarter\", col(\"Revenue_QTD\").alias(\"Revenue\"), \"Category\"))\n",
    "\n",
    "    print(\"‚úÖ Excel pivoting and QTD calculation completed.\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # STEP 4: Merge SAP data with Excel data\n",
    "    # Combines the most up-to-date SAP data with stopgap Excel-based values.\n",
    "    # This ensures historical completeness while transitioning fully to automated data.\n",
    "    final_df = (Region1_df\n",
    "                .unionByName(Region2_df)\n",
    "                .unionByName(Region1_excel_qtd)\n",
    "                .unionByName(Region2_excel_qtd))\n",
    "\n",
    "    final_df_clean = final_df.dropna(subset=[\"Date\", \"FiscalYear\", \"FiscalQuarter\", \"Revenue\", \"Category\"])\n",
    "    final_df_clean = final_df_clean.orderBy(col(\"Date\").desc())\n",
    "\n",
    "    print(\"‚úÖ Union completed successfully.\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # STEP 5: Write the unified QTD revenue to Databricks\n",
    "    # Output table is consumed by Power BI dashboards to monitor daily revenue vs. targets for Region1 and Region2. \n",
    "    final_df_casted = final_df_clean.select(\n",
    "        col(\"Date\").cast(\"date\").alias(\"Date\"),\n",
    "        col(\"FiscalYear\").cast(\"string\").alias(\"FiscalYear\"),\n",
    "        col(\"FiscalQuarter\").cast(\"string\").alias(\"FiscalQuarter\"),\n",
    "        col(\"Revenue\").cast(\"double\").alias(\"Revenue\"),\n",
    "        col(\"Category\").cast(\"string\").alias(\"Category\")\n",
    "    )\n",
    "\n",
    "    final_df_casted.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"analytics.gold.qtd_rev_r12\")\n",
    "\n",
    "    print(\"‚úÖ Delta table written successfully.\")\n",
    "\n",
    "# Error handling for unexpected failures\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Job failed with error: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# Log total duration regardless of success/failure\n",
    "finally:\n",
    "    end_time = datetime.now()\n",
    "    print(f\"üèÅ RLM Revenue Refresh Completed at {end_time}\")\n",
    "    print(f\"üïê Total Duration: {(end_time - start_time).seconds} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890b24f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
